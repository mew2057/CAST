{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark, RERUN EVERY TIME!\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '50g')\n",
    "\n",
    "conf = SparkConf()\\\n",
    "        .setMaster(\"spark://c650mnp06:7077\")\\\n",
    "        .setAppName(\"Sandbox\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Constants\n",
    "TRANS_ALLOC = \"cast-allocation-ornl\"\n",
    "TRANS_STEP  = \"cast-allocation-step-ornl\"\n",
    "STEP_HIST   = \"cast-db-csm_step_history-ornl-*\"\n",
    "NODE_HIST   = \"cast-db-csm_allocation_node_history-ornl-*\"\n",
    "ALLOC_HIST  = \"cast-db-csm_allocation_history-ornl-*\"\n",
    "\n",
    "# Set the Connection Constants\n",
    "NODES       = \"10.7.4.15,10.7.4.17\"\n",
    "PORTS       = \"9200\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Node  History\n",
    "\n",
    "We want to get some data from the allocation node history table before anything else and leverage Elasticsearch. Step 1 is to determine which fields we want and how we're going to aggregate them (tons of data is great, but skew can be just as if not more interesting) Using this we can start to refine the allocation data to make it easier to run analytics on the allocation data. The following variables are all in the **data** object. The data will be condensed to pivot on the **data.allocation_id**. \n",
    "\n",
    "**NOTE:** This is one way of analyzing the data. Other analytics may be written using the raw allocation node data, but I'm interested in a few specific indicators to apply to the overall allocation.\n",
    "\n",
    "* **memory_usage_max**\n",
    "    * Should store the Average and the Maximum values.\n",
    "    * **mem_usage_avg**\n",
    "    * **mem_usage_max**\n",
    "* **gpu_usage**\n",
    "    * Trickier, do we want the total sum or average of all nodes?\n",
    "    * **gpu_usage_avg**\n",
    "* **gpfs_read**\n",
    "    * I think the average across all nodes should be appropriate to get a sense for GPFS usage in general.\n",
    "    * **gpfs_read_avg**\n",
    "* **gpfs_write**\n",
    "    * I think the average across all nodes should be appropriate to get a sense for GPFS usage in general.\n",
    "    * **gpfs_write_avg**\n",
    "* **allocation_id**\n",
    "    * Pivot value, summations and math operations pivot on this data point.\n",
    "* **cpu_usage**\n",
    "    * Average should be fine, can show trends. If combined with median, could indicate uneven workload.\n",
    "    * **cpu_usage_avg**\n",
    "    * **cpu_usage_median**\n",
    "* **ib_tx**\n",
    "    * **ib_tx_avg** \n",
    "* **ib_rx**\n",
    "    * **ib_rx_avg** \n",
    "* **energy**\n",
    "    * **energy_avg**\n",
    "    * **energy_median**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Set up the time range\n",
    "date_format= '%Y-%m-%d'\n",
    "search_format='date_time_no_millis'\n",
    "\n",
    "start_time=\"2019-03-01T01:00:00Z\"\n",
    "end_time=\"2019-03-02T01:00:00Z\"\n",
    "\n",
    "timerange='''{{\n",
    "    \"lte\"    : \"{2}\",\n",
    "    \"gte\"    : \"{0}\",\n",
    "    \"format\" : \"{1}\"\n",
    "}}'''.format(start_time, search_format, end_time)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{ \n",
      "    \"query\" : {\n",
      "        \"range\" : {\n",
      "             \"data.history_time\" : {\n",
      "    \"lte\"    : \"2019-03-02T01:00:00Z\",\n",
      "    \"gte\"    : \"2019-03-01T01:00:00Z\",\n",
      "    \"format\" : \"date_time_no_millis\"\n",
      "}\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('C267_2kBUCN44TPi76iy',\n",
       " {'data': {'memory_usage_max': 14262140928,\n",
       "   'gpu_usage': 80018308947,\n",
       "   'gpfs_write': 43495780352,\n",
       "   'gpu_energy': 27377712,\n",
       "   'allocation_id': 192585,\n",
       "   'ib_rx': 22715380148,\n",
       "   'gpfs_read': 87735091200,\n",
       "   'cpu_usage': 228366705621402,\n",
       "   'ib_tx': 28335591197,\n",
       "   'energy': 38796902}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Write a Query!\n",
    "\n",
    "NHistQuery = '''\n",
    "{{ \n",
    "    \"query\" : {{\n",
    "        \"range\" : {{\n",
    "             \"data.history_time\" : {0}\n",
    "        }}\n",
    "    }}\n",
    "}}'''.format(timerange)\n",
    "\n",
    "#NHistQuery='{ \"query\":{ \"match_all\":{ } } }'\n",
    "\n",
    "print(NHistQuery)\n",
    "NHistFields = [\"data.allocation_id\",  \"data.ib_tx\", \"data.ib_rx\", \"data.memory_usage_max\", \"data.gpu_usage\",\n",
    "                \"data.gpfs_read\", \"data.gpfs_write\", \"data.gpu_energy\", \"data.cpu_usage\", \"data.energy\"]\n",
    "\n",
    "es_conf = {\"es.resource\": \"{0}\".format(NODE_HIST),\n",
    "          \"es.nodes\"    : NODES,\n",
    "          \"es.port\"     : PORTS,\n",
    "          \"es.query\"    : NHistQuery, \n",
    "          \"es.read.field.include\" : \",\".join(NHistFields) }\n",
    "\n",
    "\n",
    "\n",
    "nodeHist = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\",\\\n",
    "                         \"org.apache.hadoop.io.NullWritable\", \\\n",
    "                         \"org.elasticsearch.hadoop.mr.LinkedMapWritable\",conf=es_conf)\n",
    "nodeHist.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "hSchema = (  \"allocation_id\", (\"memory_usage_max\", \"gpu_usage\", \"gpfs_write\", \"cpu_usage\", \"ib_tx\", \"ib_rx\", \"energy\"))\n",
    "\n",
    "def mapObj( o ):\n",
    "    obj  = o[1][\"data\"]\n",
    "    \n",
    "    # Build the tuple, if the field is None  set to 0.\n",
    "    oTup = tuple( obj[field] if obj[field] is not None  else 0  for field in hSchema[1]  )\n",
    "    return (obj[\"allocation_id\"], oTup)\n",
    "    \n",
    "\n",
    "def reduceObj(x,y):\n",
    "    return tuple(map(add, x,y))\n",
    "      \n",
    "\n",
    "\n",
    "nHistReduced = nodeHist.map(mapObj).reduceByKey(reduceObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194418,\n",
       " (185462554624,\n",
       "  16747076,\n",
       "  62380670513152,\n",
       "  736386358996,\n",
       "  1007324318967115,\n",
       "  977201842041714,\n",
       "  7071035517))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nHistReduced.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nHistReduced.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocation Node History Reduced\n",
    "At this point we have a full allocation node history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{ \n",
      "    \"query\" : {\n",
      "        \"range\" : {\n",
      "             \"data.end_time\" : {\n",
      "    \"lte\"    : \"2019-03-02T01:00:00Z\",\n",
      "    \"gte\"    : \"2019-03-01T01:00:00Z\",\n",
      "    \"format\" : \"date_time_no_millis\"\n",
      "}\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('-ydr_WkBkJgQNSr8-OIf',\n",
       " {'data': {'projected_memory': 0,\n",
       "   'ssd_min': 0,\n",
       "   'num_processors': 0,\n",
       "   'time_limit': 7200,\n",
       "   'num_gpus': 0,\n",
       "   'user_name': 'walksloud',\n",
       "   'end_time': '2019-03-01 01:00:08.433065',\n",
       "   'launch_node_name': 'batch3',\n",
       "   'begin_time': '2019-03-01 00:59:26.029625',\n",
       "   'num_nodes': 12,\n",
       "   'allocation_id': 194312,\n",
       "   'job_submit_time': '2019-03-01 00:58:55',\n",
       "   'state': 'complete',\n",
       "   'isolated_cores': 1,\n",
       "   'ssd_max': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Write a Query!\n",
    "\n",
    "allocQuery = '''\n",
    "{{ \n",
    "    \"query\" : {{\n",
    "        \"range\" : {{\n",
    "             \"data.end_time\" : {0}\n",
    "        }}\n",
    "    }}\n",
    "}}'''.format(timerange)\n",
    "\n",
    "print(allocQuery)\n",
    "allocFields = [\"data.allocation_id\"   ,  \"data.job_submit_time\", \"data.begin_time\"    , \"data.end_time\" , \n",
    "               \"data.projected_memory\",   \"data.state\"         , \"data.ssd_min\"  , \n",
    "               \"data.ssd_max\"         , \"data.user_name\"       , \"data.num_processors\", \"data.num_nodes\", \n",
    "               \"data.launch_node_name\", \"data.time_limit\"      , \"data.isolated_cores\", \"data.num_gpus\" ]\n",
    "\n",
    "es_conf = {\"es.resource\": \"{0}\".format(ALLOC_HIST),\n",
    "          \"es.nodes\"    : NODES,\n",
    "          \"es.port\"     : PORTS,\n",
    "          \"es.query\"    : allocQuery, \n",
    "          \"es.read.field.include\" : \",\".join(allocFields) }\n",
    "\n",
    "\n",
    "\n",
    "aHist = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\",\\\n",
    "                         \"org.apache.hadoop.io.NullWritable\", \\\n",
    "                         \"org.elasticsearch.hadoop.mr.LinkedMapWritable\",conf=es_conf)\n",
    "aHist.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "allocSchema = (\"allocation_id\"    , ( \"job_submit_time\", \"begin_time\"    , \"end_time\"      ,\n",
    "               \"projected_memory\" ,   \"state\"          , \"ssd_min\"       ,\n",
    "                \"ssd_max\"         ,   \"user_name\"      , \"num_processors\", \"num_nodes\"     , \n",
    "                \"launch_node_name\",   \"time_limit\"     , \"isolated_cores\", \"num_gpus\" ))\n",
    "\n",
    "def mapAHistObj( o ):\n",
    "    obj  = o[1][\"data\"]\n",
    "    \n",
    "    # Build the tuple, if the field is None  set to 0.\n",
    "    oTup = tuple( obj[field] if obj[field] is not None  else 0  for field in allocSchema[1]  )\n",
    "    return (obj[allocSchema[0]], oTup)\n",
    "\n",
    "\n",
    "aHistReduced = aHist.map(mapAHistObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[21] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jSchema = tuple([allocSchema[0]]) + allocSchema[1] + hSchema[1]\n",
    "\n",
    "def mapJoin(obj):\n",
    "    return tuple([obj[0]]) + obj[1][0] + obj[1][1]\n",
    "\n",
    "joinedHist= aHistReduced.join(nHistReduced).map(mapJoin)\n",
    "joinedHist.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the goods.\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pyspark.ml\n",
    "\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey patch  the DataFrame to have a makeDummies function.\n",
    "def makeDummies(self, colName):\n",
    "    if self.dummyCols is None:\n",
    "        self.dummyCols = dict()\n",
    "        self.dummyCols[\"_base\"] = self.columns\n",
    "        #del self.dummyCols[\"_base\"][0]\n",
    "        \n",
    "        \n",
    "    categ = self.select(colName).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    exps  = [ F.when(F.col(colName) == cat, 1).otherwise(0).alias(str(cat)) for cat in categ ]\n",
    "    \n",
    "    self.dummyCols[colName] = categ    \n",
    "    \n",
    "    self.dummyCols[\"_base\"].remove(colName)\n",
    "    \n",
    "    newDF =  self.select( self.columns  + exps).drop(colName)\n",
    "    newDF.dummyCols = self.dummyCols\n",
    "    \n",
    "    return newDF\n",
    "\n",
    "def getDummyCols(self, *args):\n",
    "    if self.dummyCols is None:\n",
    "        self.dummyCols = dict()\n",
    "\n",
    "    if len(args) is 0:\n",
    "        args=self.dummyCols.keys()\n",
    "        \n",
    "    dCols = list()\n",
    "    for col in args :\n",
    "        dCols += self.dummyCols.get(col, [])\n",
    "        \n",
    "    return dCols\n",
    "\n",
    "def getBase(self):\n",
    "    if self.dummyCols is not None:\n",
    "        return self.dummyCols[\"_base\"]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def makeDate(self, colName):\n",
    "    self.select(colName).rdd.map(lambda d : datetime.strptime(d))\n",
    "    return self.select(self.columns)\n",
    "\n",
    "pyspark.sql.dataframe.DataFrame.makeDummies = makeDummies\n",
    "pyspark.sql.dataframe.DataFrame.dummyCols   = None\n",
    "pyspark.sql.dataframe.DataFrame.makeDate    = makeDate\n",
    "pyspark.sql.dataframe.DataFrame.getDummyCols= getDummyCols\n",
    "pyspark.sql.dataframe.DataFrame.getBase     = getBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "jDF = joinedHist.toDF(jSchema)\n",
    "jDF.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/tmp/joinDataFrame.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "uJDF = jDF.makeDate(\"job_submit_time\")\\\n",
    "    .makeDate(\"begin_time\")\\\n",
    "    .makeDate(\"end_time\")\\\n",
    "    .makeDummies(\"launch_node_name\")\\\n",
    "    .makeDummies(\"state\")\\\n",
    "    .makeDummies(\"user_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE DON'T RUN THIS TWICE, IT KILLS THE DUMMY COLUMN LOGIC!\n",
    "#uJDF.getDummyCols(\"_base\",\"user_name\", \"account\",\"launch_node_name\",\"state\",\"job_type\"))\n",
    "featureCols = uJDF.getDummyCols()\n",
    "uJDF = uJDF.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = uJDF.rdd.map(lambda row: row[4:])\n",
    "featureCols = uJDF.columns[4:]\n",
    "scaler      = StandardScaler(withMean=true, withStd=True, inputCol=\"feature\", outputCol=\"scaledFeature\")\n",
    "#features    = StandardScaler.transform(features)\n",
    "corr_matrix = Statistics.corr(features, method=\"pearson\") #  Retrieve a matrix\n",
    "pdJDF       = pd.DataFrame(corr_matrix)\n",
    "\n",
    "pdJDF.index,pdJDF.columns = featureCols,featureCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-550efc533f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "features.head().features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uJDF.getDummyCols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allocation_id',\n",
       " 'job_submit_time',\n",
       " 'begin_time',\n",
       " 'end_time',\n",
       " 'projected_memory',\n",
       " 'ssd_min',\n",
       " 'ssd_max',\n",
       " 'num_processors',\n",
       " 'num_nodes',\n",
       " 'time_limit',\n",
       " 'isolated_cores',\n",
       " 'num_gpus',\n",
       " 'memory_usage_max',\n",
       " 'gpu_usage',\n",
       " 'gpfs_write',\n",
       " 'cpu_usage',\n",
       " 'ib_tx',\n",
       " 'ib_rx',\n",
       " 'energy',\n",
       " 'batch1',\n",
       " 'batch4',\n",
       " 'batch2',\n",
       " 'batch5',\n",
       " 'batch3',\n",
       " 'complete',\n",
       " 'mcclurej',\n",
       " 'pfliu',\n",
       " 'esuchyta',\n",
       " 'merzky1',\n",
       " 'psvirin',\n",
       " 'brenaud',\n",
       " 'kmehta',\n",
       " 'mpapadak',\n",
       " 'shku',\n",
       " 'rta',\n",
       " 'cnegre',\n",
       " 'wqzhang',\n",
       " 'gustavj',\n",
       " 'walkup',\n",
       " 'q5p',\n",
       " 'tmaier',\n",
       " 'peller',\n",
       " 'msandov1',\n",
       " 'wanghy',\n",
       " 'rsankar',\n",
       " 'bee',\n",
       " 'wangy',\n",
       " 'nwedi',\n",
       " 'maxpkatz',\n",
       " 'jaharris',\n",
       " 'panitkin',\n",
       " 'jyc',\n",
       " 'hshan',\n",
       " 'z8j',\n",
       " 'candy',\n",
       " 'ia4021',\n",
       " 'cosdis',\n",
       " 'xsy',\n",
       " 'brettin',\n",
       " 'kiran92',\n",
       " 'dappelh',\n",
       " 'kngott',\n",
       " 'havenith',\n",
       " 'amueller',\n",
       " 'gbarca',\n",
       " 'bvilasen',\n",
       " 'luissua',\n",
       " 'tjcw',\n",
       " 'lld',\n",
       " 'carson16',\n",
       " 'naughton',\n",
       " 'kathirrk',\n",
       " 'pgrete',\n",
       " 'chunli',\n",
       " 'arghyac',\n",
       " 'khv',\n",
       " 'azamat',\n",
       " 'ames',\n",
       " 'o0d',\n",
       " 'tpapathe',\n",
       " 'daddison',\n",
       " 'glaser',\n",
       " 'jbao',\n",
       " 'dwright2',\n",
       " 'hancheng',\n",
       " 'rameshp',\n",
       " 'enielsen',\n",
       " 'ngawande',\n",
       " 'awlauria',\n",
       " 'chulwoo',\n",
       " 'ayenpure',\n",
       " 'jvant',\n",
       " 'mthevenet',\n",
       " 'psteinbr',\n",
       " 'thom13',\n",
       " 'lot',\n",
       " 'pwjones',\n",
       " 'etprates',\n",
       " 'elbriggs',\n",
       " 'akorzun',\n",
       " 'sabbott',\n",
       " 'tdodd3',\n",
       " 'div',\n",
       " 'mlupopa',\n",
       " 'travisj',\n",
       " 'alanger1',\n",
       " 'ashan',\n",
       " 'frivas',\n",
       " 'jackm',\n",
       " 'dsambit',\n",
       " 'dtgraves',\n",
       " 'richford',\n",
       " 'vivekb',\n",
       " 'yeluo',\n",
       " 'aayala',\n",
       " 'scheinberg',\n",
       " 'ncollier',\n",
       " 'yaomin',\n",
       " 'shijin',\n",
       " 'gabrielgaz',\n",
       " 'ly15rice',\n",
       " 'crjones',\n",
       " 'junqi',\n",
       " 'orsvuran',\n",
       " 'jiawl',\n",
       " 'wozniak',\n",
       " 'walksloud',\n",
       " 'bjoo']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['allocation_id',\n",
       " 'job_submit_time',\n",
       " 'begin_time',\n",
       " 'end_time',\n",
       " 'projected_memory',\n",
       " 'ssd_min',\n",
       " 'ssd_max',\n",
       " 'num_processors',\n",
       " 'num_nodes',\n",
       " 'time_limit',\n",
       " 'isolated_cores',\n",
       " 'num_gpus',\n",
       " 'memory_usage_max',\n",
       " 'gpu_usage',\n",
       " 'gpfs_write',\n",
       " 'cpu_usage',\n",
       " 'ib_tx',\n",
       " 'ib_rx',\n",
       " 'energy',\n",
       " 'batch1',\n",
       " 'batch4',\n",
       " 'batch2',\n",
       " 'batch5',\n",
       " 'batch3',\n",
       " 'complete',\n",
       " 'mcclurej',\n",
       " 'pfliu',\n",
       " 'esuchyta',\n",
       " 'merzky1',\n",
       " 'psvirin',\n",
       " 'brenaud',\n",
       " 'kmehta',\n",
       " 'mpapadak',\n",
       " 'shku',\n",
       " 'rta',\n",
       " 'cnegre',\n",
       " 'wqzhang',\n",
       " 'gustavj',\n",
       " 'walkup',\n",
       " 'q5p',\n",
       " 'tmaier',\n",
       " 'peller',\n",
       " 'msandov1',\n",
       " 'wanghy',\n",
       " 'rsankar',\n",
       " 'bee',\n",
       " 'wangy',\n",
       " 'nwedi',\n",
       " 'maxpkatz',\n",
       " 'jaharris',\n",
       " 'panitkin',\n",
       " 'jyc',\n",
       " 'hshan',\n",
       " 'z8j',\n",
       " 'candy',\n",
       " 'ia4021',\n",
       " 'cosdis',\n",
       " 'xsy',\n",
       " 'brettin',\n",
       " 'kiran92',\n",
       " 'dappelh',\n",
       " 'kngott',\n",
       " 'havenith',\n",
       " 'amueller',\n",
       " 'gbarca',\n",
       " 'bvilasen',\n",
       " 'luissua',\n",
       " 'tjcw',\n",
       " 'lld',\n",
       " 'carson16',\n",
       " 'naughton',\n",
       " 'kathirrk',\n",
       " 'pgrete',\n",
       " 'chunli',\n",
       " 'arghyac',\n",
       " 'khv',\n",
       " 'azamat',\n",
       " 'ames',\n",
       " 'o0d',\n",
       " 'tpapathe',\n",
       " 'daddison',\n",
       " 'glaser',\n",
       " 'jbao',\n",
       " 'dwright2',\n",
       " 'hancheng',\n",
       " 'rameshp',\n",
       " 'enielsen',\n",
       " 'ngawande',\n",
       " 'awlauria',\n",
       " 'chulwoo',\n",
       " 'ayenpure',\n",
       " 'jvant',\n",
       " 'mthevenet',\n",
       " 'psteinbr',\n",
       " 'thom13',\n",
       " 'lot',\n",
       " 'pwjones',\n",
       " 'etprates',\n",
       " 'elbriggs',\n",
       " 'akorzun',\n",
       " 'sabbott',\n",
       " 'tdodd3',\n",
       " 'div',\n",
       " 'mlupopa',\n",
       " 'travisj',\n",
       " 'alanger1',\n",
       " 'ashan',\n",
       " 'frivas',\n",
       " 'jackm',\n",
       " 'dsambit',\n",
       " 'dtgraves',\n",
       " 'richford',\n",
       " 'vivekb',\n",
       " 'yeluo',\n",
       " 'aayala',\n",
       " 'scheinberg',\n",
       " 'ncollier',\n",
       " 'yaomin',\n",
       " 'shijin',\n",
       " 'gabrielgaz',\n",
       " 'ly15rice',\n",
       " 'crjones',\n",
       " 'junqi',\n",
       " 'orsvuran',\n",
       " 'jiawl',\n",
       " 'wozniak',\n",
       " 'walksloud',\n",
       " 'bjoo']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uJDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
