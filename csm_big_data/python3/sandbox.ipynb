{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark, RERUN EVERY TIME!\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "conf = SparkConf()\\\n",
    "        .setMaster(\"local\")\\\n",
    "        .setAppName(\"Sandbox\")\n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Constants\n",
    "TRANS_ALLOC = \"cast-allocation-ornl\"\n",
    "TRANS_STEP  = \"cast-allocation-step-ornl\"\n",
    "STEP_HIST   = \"cast-db-csm_step_history-ornl-*\"\n",
    "NODE_HIST   = \"cast-db-csm_allocation_node_history-ornl-*\"\n",
    "ALLOC_HIST  = \"cast-db-csm_allocation_history-ornl-*\"\n",
    "\n",
    "# Set the Connection Constants\n",
    "NODES       = \"10.7.4.15,10.7.4.17\"\n",
    "PORTS       = \"9200\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Node  History\n",
    "\n",
    "We want to get some data from the allocation node history table before anything else and leverage Elasticsearch. Step 1 is to determine which fields we want and how we're going to aggregate them (tons of data is great, but skew can be just as if not more interesting) Using this we can start to refine the allocation data to make it easier to run analytics on the allocation data. The following variables are all in the **data** object. The data will be condensed to pivot on the **data.allocation_id**. \n",
    "\n",
    "**NOTE:** This is one way of analyzing the data. Other analytics may be written using the raw allocation node data, but I'm interested in a few specific indicators to apply to the overall allocation.\n",
    "\n",
    "* **memory_usage_max**\n",
    "    * Should store the Average and the Maximum values.\n",
    "    * **mem_usage_avg**\n",
    "    * **mem_usage_max**\n",
    "* **gpu_usage**\n",
    "    * Trickier, do we want the total sum or average of all nodes?\n",
    "    * **gpu_usage_avg**\n",
    "* **gpfs_read**\n",
    "    * I think the average across all nodes should be appropriate to get a sense for GPFS usage in general.\n",
    "    * **gpfs_read_avg**\n",
    "* **gpfs_write**\n",
    "    * I think the average across all nodes should be appropriate to get a sense for GPFS usage in general.\n",
    "    * **gpfs_write_avg**\n",
    "* **allocation_id**\n",
    "    * Pivot value, summations and math operations pivot on this data point.\n",
    "* **cpu_usage**\n",
    "    * Average should be fine, can show trends. If combined with median, could indicate uneven workload.\n",
    "    * **cpu_usage_avg**\n",
    "    * **cpu_usage_median**\n",
    "* **ib_tx**\n",
    "    * **ib_tx_avg** \n",
    "* **ib_rx**\n",
    "    * **ib_rx_avg** \n",
    "* **energy**\n",
    "    * **energy_avg**\n",
    "    * **energy_median**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: java.lang.IllegalArgumentException: Failed to parse query: { \"query\" :\n    {\n        \"match_all\": {data.allocation_id\"}\n    },\n    \"aggs\":{\n        \"histogram\": {\n            \"field\" : \"data.allocation_id\",\n            \"interval\" : 1\n        }\n    },\n    \"size\" : 0\n}\n\tat org.elasticsearch.hadoop.rest.query.QueryUtils.parseQuery(QueryUtils.java:59)\n\tat org.elasticsearch.hadoop.rest.query.QueryUtils.parseQueryAndFilters(QueryUtils.java:81)\n\tat org.elasticsearch.hadoop.rest.RestService.findSlicePartitions(RestService.java:324)\n\tat org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:270)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:414)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:395)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:302)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.codehaus.jackson.JsonParseException: Unexpected character ('d' (code 100)): was expecting double-quote to start field name\n at [Source: java.io.StringReader@2439fb25; line: 3, column: 24]\n\tat org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)\n\tat org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)\n\tat org.codehaus.jackson.impl.JsonParserMinimalBase._reportUnexpectedChar(JsonParserMinimalBase.java:442)\n\tat org.codehaus.jackson.impl.ReaderBasedParser._handleUnusualFieldName(ReaderBasedParser.java:1084)\n\tat org.codehaus.jackson.impl.ReaderBasedParser._parseFieldName(ReaderBasedParser.java:977)\n\tat org.codehaus.jackson.impl.ReaderBasedParser.nextToken(ReaderBasedParser.java:418)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:187)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:47)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:196)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:47)\n\tat org.codehaus.jackson.map.deser.std.MapDeserializer._readAndBind(MapDeserializer.java:319)\n\tat org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize(MapDeserializer.java:249)\n\tat org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize(MapDeserializer.java:33)\n\tat org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2732)\n\tat org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1863)\n\tat org.elasticsearch.hadoop.rest.query.RawQueryBuilder.<init>(RawQueryBuilder.java:58)\n\tat org.elasticsearch.hadoop.rest.query.SimpleQueryParser.parse(SimpleQueryParser.java:51)\n\tat org.elasticsearch.hadoop.rest.query.QueryUtils.parseQuery(QueryUtils.java:57)\n\t... 34 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-892927aa15d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m nodeHist = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\",\\\n\u001b[1;32m     23\u001b[0m                          \u001b[0;34m\"org.apache.hadoop.io.NullWritable\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                          \"org.elasticsearch.hadoop.mr.LinkedMapWritable\",conf=es_conf)\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36mnewAPIHadoopRDD\u001b[0;34m(self, inputFormatClass, keyClass, valueClass, keyConverter, valueConverter, conf, batchSize)\u001b[0m\n\u001b[1;32m    767\u001b[0m         jrdd = self._jvm.PythonRDD.newAPIHadoopRDD(self._jsc, inputFormatClass, keyClass,\n\u001b[1;32m    768\u001b[0m                                                    \u001b[0mvalueClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m                                                    jconf, batchSize)\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.\n: java.lang.IllegalArgumentException: Failed to parse query: { \"query\" :\n    {\n        \"match_all\": {data.allocation_id\"}\n    },\n    \"aggs\":{\n        \"histogram\": {\n            \"field\" : \"data.allocation_id\",\n            \"interval\" : 1\n        }\n    },\n    \"size\" : 0\n}\n\tat org.elasticsearch.hadoop.rest.query.QueryUtils.parseQuery(QueryUtils.java:59)\n\tat org.elasticsearch.hadoop.rest.query.QueryUtils.parseQueryAndFilters(QueryUtils.java:81)\n\tat org.elasticsearch.hadoop.rest.RestService.findSlicePartitions(RestService.java:324)\n\tat org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:270)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:414)\n\tat org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:395)\n\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:302)\n\tat org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.codehaus.jackson.JsonParseException: Unexpected character ('d' (code 100)): was expecting double-quote to start field name\n at [Source: java.io.StringReader@2439fb25; line: 3, column: 24]\n\tat org.codehaus.jackson.JsonParser._constructError(JsonParser.java:1433)\n\tat org.codehaus.jackson.impl.JsonParserMinimalBase._reportError(JsonParserMinimalBase.java:521)\n\tat org.codehaus.jackson.impl.JsonParserMinimalBase._reportUnexpectedChar(JsonParserMinimalBase.java:442)\n\tat org.codehaus.jackson.impl.ReaderBasedParser._handleUnusualFieldName(ReaderBasedParser.java:1084)\n\tat org.codehaus.jackson.impl.ReaderBasedParser._parseFieldName(ReaderBasedParser.java:977)\n\tat org.codehaus.jackson.impl.ReaderBasedParser.nextToken(ReaderBasedParser.java:418)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:187)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:47)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:196)\n\tat org.codehaus.jackson.map.deser.std.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:47)\n\tat org.codehaus.jackson.map.deser.std.MapDeserializer._readAndBind(MapDeserializer.java:319)\n\tat org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize(MapDeserializer.java:249)\n\tat org.codehaus.jackson.map.deser.std.MapDeserializer.deserialize(MapDeserializer.java:33)\n\tat org.codehaus.jackson.map.ObjectMapper._readMapAndClose(ObjectMapper.java:2732)\n\tat org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:1863)\n\tat org.elasticsearch.hadoop.rest.query.RawQueryBuilder.<init>(RawQueryBuilder.java:58)\n\tat org.elasticsearch.hadoop.rest.query.SimpleQueryParser.parse(SimpleQueryParser.java:51)\n\tat org.elasticsearch.hadoop.rest.query.QueryUtils.parseQuery(QueryUtils.java:57)\n\t... 34 more\n"
     ]
    }
   ],
   "source": [
    "sc.setLogLevel(\"DEBUG\")\n",
    "# Let's Write a Query!\n",
    "\n",
    "NHistQuery = '''{ \"query\" :\n",
    "    {\n",
    "        \"match_all\": {\"data.allocation_id\"}\n",
    "    },\n",
    "    \"aggs\":{\n",
    "        \"histogram\": {\n",
    "            \"field\" : \"data.allocation_id\",\n",
    "            \"interval\" : 1\n",
    "        }\n",
    "    },\n",
    "    \"size\" : 0\n",
    "}'''\n",
    "\n",
    "es_conf = {\"es.resource\": \"{0}\".format(NODE_HIST),\n",
    "          \"es.nodes\"    : NODES,\n",
    "          \"es.port\"     : PORTS,\n",
    "          \"es.query\"    : NHistQuery, }\n",
    "\n",
    "nodeHist = sc.newAPIHadoopRDD(\"org.elasticsearch.hadoop.mr.EsInputFormat\",\\\n",
    "                         \"org.apache.hadoop.io.NullWritable\", \\\n",
    "                         \"org.elasticsearch.hadoop.mr.LinkedMapWritable\",conf=es_conf)\n",
    "\n",
    "\n",
    "nodeHist.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#nodeHist.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeHist.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
